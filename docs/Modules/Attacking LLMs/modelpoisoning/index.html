<!doctype html>
<html lang="zh" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Modules/Attacking LLMs/modelpoisoning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Data Integrity &amp; Model Poisoning | TryHackMe-CN</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning"><meta data-rh="true" property="og:locale" content="zh"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh"><meta data-rh="true" name="docsearch:language" content="zh"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Data Integrity &amp; Model Poisoning | TryHackMe-CN"><meta data-rh="true" name="description" content="Task 1 Introduction"><meta data-rh="true" property="og:description" content="Task 1 Introduction"><link data-rh="true" rel="icon" href="/TryHackMe-CN/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning"><link data-rh="true" rel="alternate" href="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/en/docs/Modules/Attacking LLMs/modelpoisoning" hreflang="en"><link data-rh="true" rel="alternate" href="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning" hreflang="zh"><link data-rh="true" rel="alternate" href="https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"模块","item":"https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/"},{"@type":"ListItem","position":2,"name":"攻击大型语言模型","item":"https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/"},{"@type":"ListItem","position":3,"name":"Data Integrity & Model Poisoning","item":"https://tryhackmyoffsecbox.github.io/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning"}]}</script><link rel="alternate" type="application/rss+xml" href="/TryHackMe-CN/blog/rss.xml" title="TryHackMe-CN RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/TryHackMe-CN/blog/atom.xml" title="TryHackMe-CN Atom Feed"><link rel="stylesheet" href="/TryHackMe-CN/assets/css/styles.6c4c8a38.css">
<script src="/TryHackMe-CN/assets/js/runtime~main.ae094ceb.js" defer="defer"></script>
<script src="/TryHackMe-CN/assets/js/main.4139ffc9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/TryHackMe-CN/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/TryHackMe-CN/"><div class="navbar__logo"><img src="/TryHackMe-CN/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/TryHackMe-CN/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">TryHackMe-CN</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/TryHackMe-CN/docs/Modules/">Tutorial</a><a class="navbar__item navbar__link" href="/TryHackMe-CN/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/TryHackMe-CN/en/docs/Modules/Attacking LLMs/modelpoisoning" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh">简体中文</a></li></ul></div><a href="https://github.com/TryHackMyOffsecBox/TryHackMe-CN" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="切换浅色/暗黑模式（当前为system mode）"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/TryHackMe-CN/docs/Modules/"><span title="模块" class="categoryLinkLabel_W154">模块</span></a><button aria-label="折叠侧边栏分类 &#x27;模块&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/"><span title="攻击大型语言模型" class="categoryLinkLabel_W154">攻击大型语言模型</span></a><button aria-label="折叠侧边栏分类 &#x27;攻击大型语言模型&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/inputmanipulationpromptinjection"><span title="输入操作与提示注入" class="linkLabel_WmDU">输入操作与提示注入</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/outputhandlingandprivacyrisks"><span title="大型语言模型输出处理与隐私风险" class="linkLabel_WmDU">大型语言模型输出处理与隐私风险</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/modelpoisoning"><span title="Data Integrity &amp; Model Poisoning" class="linkLabel_WmDU">Data Integrity &amp; Model Poisoning</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Command Line/"><span title="命令行" class="categoryLinkLabel_W154">命令行</span></a><button aria-label="展开侧边栏分类 &#x27;命令行&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Container Security/"><span title="容器安全" class="categoryLinkLabel_W154">容器安全</span></a><button aria-label="展开侧边栏分类 &#x27;容器安全&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Host Evasions/"><span title="主机规避" class="categoryLinkLabel_W154">主机规避</span></a><button aria-label="展开侧边栏分类 &#x27;主机规避&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Memory Analysis/"><span title="内存分析" class="categoryLinkLabel_W154">内存分析</span></a><button aria-label="展开侧边栏分类 &#x27;内存分析&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Microsoft Defender XDR/"><span title="Microsoft Defender XDR" class="categoryLinkLabel_W154">Microsoft Defender XDR</span></a><button aria-label="展开侧边栏分类 &#x27;Microsoft Defender XDR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/TryHackMe-CN/docs/Modules/Starters/"><span title="入门挑战" class="categoryLinkLabel_W154">入门挑战</span></a><button aria-label="展开侧边栏分类 &#x27;入门挑战&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/TryHackMe-CN/docs/Networks/"><span title="网络" class="categoryLinkLabel_W154">网络</span></a><button aria-label="展开侧边栏分类 &#x27;网络&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/TryHackMe-CN/docs/"><span title="index" class="linkLabel_WmDU">index</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/TryHackMe-CN/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/TryHackMe-CN/docs/Modules/"><span>模块</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/"><span>攻击大型语言模型</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Data Integrity &amp; Model Poisoning</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>Data Integrity &amp; Model Poisoning</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-1-introduction">Task 1 Introduction<a href="#task-1-introduction" class="hash-link" aria-label="Task 1 Introduction的直接链接" title="Task 1 Introduction的直接链接" translate="no">​</a></h2>
<p>Modern AI systems depend heavily on the quality and trustworthiness of their data and model components. When attackers compromise training data or model parameters, they can inject hidden vulnerabilities, manipulate predictions, or bias outputs. In this room, you&#x27;ll explore how these attacks work and how to detect and mitigate them using practical techniques.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Learning Objectives的直接链接" title="Learning Objectives的直接链接" translate="no">​</a></h3>
<ul>
<li class="">Understand how compromised datasets or model components can lead to security risks.</li>
<li class="">Examine common ways adversaries use to introduce malicious inputs during training or fine-tuning.</li>
<li class="">Assess vulnerabilities in externally sourced datasets, pre-trained models, and third-party libraries.</li>
<li class="">Practice model poisoning through the eyes of an attacker.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接" translate="no">​</a></h3>
<p>Data integrity and model poisoning are specialised threats within the broader field of machine learning security. To get the most out of this room, you should have a foundational understanding of how machine learning models are trained and deployed, as well as the basics of data preprocessing and model evaluation. Additionally, you should be familiar with general security principles related to supply chain and input validation.</p>
<ul>
<li class=""><a href="https://tryhackme.com/room/aimlsecuritythreats" target="_blank" rel="noopener noreferrer" class="">AI/ML Security Threats</a></li>
<li class=""><a href="https://tryhackme.com/room/idadversarialattacks" target="_blank" rel="noopener noreferrer" class="">Detecting Adversarial Attacks</a></li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Answer the questions below</div><div class="admonitionContent_BuS1"><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> I have successfully started the machine. </summary><div><div class="collapsibleContent_i85q"><div class="language-plaintext codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plaintext codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">No answer needed</span><br></span></code></pre></div></div></div></div></details></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-2-supply-chain-attack">Task 2 Supply Chain Attack<a href="#task-2-supply-chain-attack" class="hash-link" aria-label="Task 2 Supply Chain Attack的直接链接" title="Task 2 Supply Chain Attack的直接链接" translate="no">​</a></h2>
<p>In this task, we will explore how attackers exploit the supply chain (termed LLM03 in the <a href="https://genai.owasp.org/llmrisk/llm032025-supply-chain/" target="_blank" rel="noopener noreferrer" class="">OWASP GenAI Security Project</a>) to attack LLMs. In the context of LLM, the supply chain refers to all the external components, datasets, model weights, adapters, libraries, and infrastructure that go into training, fine-tuning, or deploying an LLM. Because many of these pieces come from third parties or open-source repositories, they create a broad attack surface where malicious actors can tamper with inputs long before a model reaches production.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-it-occurs">How It Occurs<a href="#how-it-occurs" class="hash-link" aria-label="How It Occurs的直接链接" title="How It Occurs的直接链接" translate="no">​</a></h3>
<ul>
<li class="">Attackers tamper with or &quot;poison&quot; external components used by LLM systems like pre-trained model weights, fine-tuning adapters, datasets, or third-party libraries.</li>
<li class="">Weak provenance (e.g., poor source documentation and lack of integrity verification) makes detection harder. Attackers can disguise malicious components so that they pass standard benchmarks yet introduce hidden backdoors.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="An image of an AI response being poisoned through an untrusted data source" src="/TryHackMe-CN/assets/images/image_20251202-230237-c84d24a728b5062dc1b824abdb684244.png" width="818" height="572" class="img_ev3q"></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="major-real-world-cases">Major Real-World Cases<a href="#major-real-world-cases" class="hash-link" aria-label="Major Real-World Cases的直接链接" title="Major Real-World Cases的直接链接" translate="no">​</a></h3>
<ul>
<li class=""><strong>PoisonGPT / GPT-J-6B Compromised Version</strong>: Researchers modified an open-source model (GPT-J-6B) to include misinformation behaviour (spread fake news) while keeping it performing well on standard benchmarks. The malicious version was uploaded to Hugging Face under a name meant to look like a trusted one (typosquatting/impersonation). The modified model passed many common evaluation benchmarks almost identically to the unmodified one, so detection via standard evaluation was nearly impossible.</li>
<li class=""><a href="https://arxiv.org/abs/2401.15883" target="_blank" rel="noopener noreferrer" class="">Backdooring Pre-trained Models with Embedding Indistinguishability</a>: In this academic work, adversaries embed backdoors into pre-trained models, allowing downstream tasks to inherit the malicious behaviour. These backdoors are designed so that the poisoned embeddings are nearly indistinguishable from clean ones before and after fine-tuning. The experiment successfully triggered the backdoor under various conditions, highlighting how supply chain poisoning in the model weights can propagate.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="common-examples">Common Examples<a href="#common-examples" class="hash-link" aria-label="Common Examples的直接链接" title="Common Examples的直接链接" translate="no">​</a></h3>
<table><thead><tr><th style="text-align:left">Threat Type</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">Vulnerable or outdated packages/libraries</td><td style="text-align:left">Using old versions of ML frameworks, data pipelines, or dependencies with known vulnerabilities can allow attackers to gain entry or inject malicious behaviour. E.g., a compromised PyTorch or TensorFlow component used in fine-tuning or data preprocessing.</td></tr><tr><td style="text-align:left">Malicious pre-trained models or adapters</td><td style="text-align:left">A provider or attacker publishes a model or adapter that appears legitimate, but includes hidden malicious behaviour or bias. When downstream users use them without verifying integrity, they inherit the threat.</td></tr><tr><td style="text-align:left">Stealthy backdoor/trigger insertion</td><td style="text-align:left">The insertion of triggers that only activate under certain conditions, remaining dormant otherwise, so they evade regular testing. For example, &quot;hidden triggers&quot; in model parameters or in embeddings, which only manifest when a specific token or pattern is used.</td></tr><tr><td style="text-align:left">Collaborative/merged models</td><td style="text-align:left">Components may come from different sources, with models being merged (from multiple contributors) or using shared pipelines. Attackers may target weak links (e.g. a library or adapter) in the pipeline to introduce malicious code or backdoors.</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Answer the questions below</div><div class="admonitionContent_BuS1"><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> What is the name of the website where the malicious version of GPT-J-6B was uploaded? </summary><div><div class="collapsibleContent_i85q"><div class="language-plaintext codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plaintext codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Hugging Face</span><br></span></code></pre></div></div></div></div></details><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> What term refers to all the <strong>external</strong> components, datasets, model weights, adapters, libraries, and infrastructure used to train, fine-tune, or deploy an LLM? </summary><div><div class="collapsibleContent_i85q"><div class="language-plaintext codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plaintext codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Supply Chain</span><br></span></code></pre></div></div></div></div></details></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-3-model-poisoning">Task 3 Model Poisoning<a href="#task-3-model-poisoning" class="hash-link" aria-label="Task 3 Model Poisoning的直接链接" title="Task 3 Model Poisoning的直接链接" translate="no">​</a></h2>
<p>Model poisoning is an adversarial technique where attackers deliberately inject malicious or manipulated data during a model’s training or retraining cycle. The goal is to bias the model’s behaviour, degrade its performance, or embed hidden backdoors that can be triggered later. Unlike prompt injection, this targets the model weights, making the compromise persistent.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="prerequisite-of-model-poisoning">Prerequisite of Model Poisoning<a href="#prerequisite-of-model-poisoning" class="hash-link" aria-label="Prerequisite of Model Poisoning的直接链接" title="Prerequisite of Model Poisoning的直接链接" translate="no">​</a></h3>
<p>Model poisoning isn’t possible on every system. It specifically affects models that accept user input as part of their continuous learning or fine-tuning pipeline. For example, recommender systems, chatbots, or any adaptive model that automatically re-train on user feedback or submitted content. Static, fully offline models (where training is frozen and never updated from external inputs) are generally not vulnerable. For an attack to succeed, the model must adhere to the following:</p>
<ul>
<li class="">Incorporate untrusted user data into its training corpus.</li>
<li class="">Lack rigorous data validation.</li>
<li class="">Redeploy updated weights without strong integrity checks.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cheat-sheet-for-pentesters">Cheat Sheet for Pentesters<a href="#cheat-sheet-for-pentesters" class="hash-link" aria-label="Cheat Sheet for Pentesters的直接链接" title="Cheat Sheet for Pentesters的直接链接" translate="no">​</a></h3>
<p>Here is the checklist for red teamers and pentesters when assessing model poisoning risks:</p>
<ul>
<li class=""><strong>Data ingestion pipeline</strong>: Does the LLM or system retrain on unverified user inputs, feedback, or uploaded content?</li>
<li class=""><strong>Update frequency</strong>: How often is the model fine-tuned or updated?</li>
<li class=""><strong>Data provenance and sanitisation</strong>: Can training data sources be traced, and are they validated against poisoning attempts?</li>
<li class=""><strong>Access controls</strong>: Who can submit data included in re-training, and is that channel exposed to untrusted users?</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image of LLM attack cycle" src="/TryHackMe-CN/assets/images/image_20251214-231442-dbad4cbff310260e5f6ba80e4330f849.png" width="1090" height="765" class="img_ev3q"></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="attack-process">Attack Process<a href="#attack-process" class="hash-link" aria-label="Attack Process的直接链接" title="Attack Process的直接链接" translate="no">​</a></h3>
<ul>
<li class=""><strong>Where</strong>: Poisoning can occur at different stages, during pre-training (large-scale dataset poisoning), fine-tuning (targeted task manipulation), or continual learning (live re-training from user data).</li>
<li class=""><strong>How</strong>: The attacker seeds malicious examples into the training set, waits for the re-training cycle, and leverages the altered model behaviour for backdoors.</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Answer the questions below</div><div class="admonitionContent_BuS1"><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> An adversarial technique where attackers deliberately inject malicious or manipulated data during a model’s training is called? </summary><div><div class="collapsibleContent_i85q"><div class="language-plaintext codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-plaintext codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Model poisoning</span><br></span></code></pre></div></div></div></div></details></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-4-model-poisoning---challenge">Task 4 Model Poisoning - Challenge<a href="#task-4-model-poisoning---challenge" class="hash-link" aria-label="Task 4 Model Poisoning - Challenge的直接链接" title="Task 4 Model Poisoning - Challenge的直接链接" translate="no">​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-5-mitigation-measures">Task 5 Mitigation Measures<a href="#task-5-mitigation-measures" class="hash-link" aria-label="Task 5 Mitigation Measures的直接链接" title="Task 5 Mitigation Measures的直接链接" translate="no">​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="task-6-conclusion">Task 6 Conclusion<a href="#task-6-conclusion" class="hash-link" aria-label="Task 6 Conclusion的直接链接" title="Task 6 Conclusion的直接链接" translate="no">​</a></h2></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/TryHackMyOffsecBox/TryHackMe-CN/tree/main/packages/create-docusaurus/templates/shared/docs/Modules/Attacking LLMs/modelpoisoning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/TryHackMe-CN/docs/Modules/Attacking LLMs/outputhandlingandprivacyrisks"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">大型语言模型输出处理与隐私风险</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/TryHackMe-CN/docs/Modules/Command Line/"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">命令行</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#task-1-introduction" class="table-of-contents__link toc-highlight">Task 1 Introduction</a><ul><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li></ul></li><li><a href="#task-2-supply-chain-attack" class="table-of-contents__link toc-highlight">Task 2 Supply Chain Attack</a><ul><li><a href="#how-it-occurs" class="table-of-contents__link toc-highlight">How It Occurs</a></li><li><a href="#major-real-world-cases" class="table-of-contents__link toc-highlight">Major Real-World Cases</a></li><li><a href="#common-examples" class="table-of-contents__link toc-highlight">Common Examples</a></li></ul></li><li><a href="#task-3-model-poisoning" class="table-of-contents__link toc-highlight">Task 3 Model Poisoning</a><ul><li><a href="#prerequisite-of-model-poisoning" class="table-of-contents__link toc-highlight">Prerequisite of Model Poisoning</a></li><li><a href="#cheat-sheet-for-pentesters" class="table-of-contents__link toc-highlight">Cheat Sheet for Pentesters</a></li><li><a href="#attack-process" class="table-of-contents__link toc-highlight">Attack Process</a></li></ul></li><li><a href="#task-4-model-poisoning---challenge" class="table-of-contents__link toc-highlight">Task 4 Model Poisoning - Challenge</a></li><li><a href="#task-5-mitigation-measures" class="table-of-contents__link toc-highlight">Task 5 Mitigation Measures</a></li><li><a href="#task-6-conclusion" class="table-of-contents__link toc-highlight">Task 6 Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/TryHackMe-CN/docs">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/TryHackMyOffsecBox" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github - TryHackMyOffsecBox<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/CRONUS-Security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github - CRONUS-Security<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 TryHackMyOffsecBox. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>