"use strict";(self.webpackChunktryhackme_cn=self.webpackChunktryhackme_cn||[]).push([[2160],{2448:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"Modules/Attacking LLMs/index","title":"Attacking LLMs","description":"Learn to identify and exploit LLM vulnerabilities, covering prompt injection, insecure output handling, and model poisoning.","source":"@site/docs/Modules/Attacking LLMs/index.md","sourceDirName":"Modules/Attacking LLMs","slug":"/Modules/Attacking LLMs/","permalink":"/TryHackMe-CN/en/docs/Modules/Attacking LLMs/","draft":false,"unlisted":false,"editUrl":"https://github.com/TryHackMyOffsecBox/TryHackMe-CN/tree/main/packages/create-docusaurus/templates/shared/docs/Modules/Attacking LLMs/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Modules","permalink":"/TryHackMe-CN/en/docs/Modules/"},"next":{"title":"Input Manipulation & Prompt Injection","permalink":"/TryHackMe-CN/en/docs/Modules/Attacking LLMs/inputmanipulationpromptinjection"}}');var a=t(4848),s=t(8453);const o={},r="Attacking LLMs",c={},l=[{value:"Input Manipulation &amp; Prompt Injection",id:"input-manipulation--prompt-injection",level:2},{value:"LLM Output Handling and Privacy Risks",id:"llm-output-handling-and-privacy-risks",level:2},{value:"Data Integrity &amp; Model Poisoning",id:"data-integrity--model-poisoning",level:2},{value:"Juicy",id:"juicy",level:2},{value:"BankGPT",id:"bankgpt",level:2},{value:"HealthGPT",id:"healthgpt",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",p:"p",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"attacking-llms",children:"Attacking LLMs"})}),"\n",(0,a.jsx)(e.p,{children:"Learn to identify and exploit LLM vulnerabilities, covering prompt injection, insecure output handling, and model poisoning."}),"\n",(0,a.jsx)(e.p,{children:"In this module, we cover practical attacks against systems that use large language models, including prompt injection, unsafe output handling, and model poisoning . You will learn how crafted inputs and careless handling of model output can expose secrets or trigger unauthorised actions, and how poisoned training data can cause persistent failures. Each topic includes hands-on exercises and realistic scenarios that show how small issues can be linked into larger attack paths. By the end, participants can build concise proof of concept attacks and suggest clear, practical mitigations."}),"\n",(0,a.jsx)(e.h2,{id:"input-manipulation--prompt-injection",children:(0,a.jsx)(e.a,{href:"/TryHackMe-CN/en/docs/Modules/Attacking%20LLMs/inputmanipulationpromptinjection",children:"Input Manipulation & Prompt Injection"})}),"\n",(0,a.jsx)(e.p,{children:"Understand the basics of LLM Prompt Injection attacks."}),"\n",(0,a.jsx)(e.h2,{id:"llm-output-handling-and-privacy-risks",children:"LLM Output Handling and Privacy Risks"}),"\n",(0,a.jsx)(e.p,{children:"Learn how LLMs handle their output and the privacy risks behind it."}),"\n",(0,a.jsx)(e.h2,{id:"data-integrity--model-poisoning",children:"Data Integrity & Model Poisoning"}),"\n",(0,a.jsx)(e.p,{children:"Understand how supply chain and model poisoning attacks can corrupt the underlying LLM."}),"\n",(0,a.jsx)(e.h2,{id:"juicy",children:"Juicy"}),"\n",(0,a.jsx)(e.p,{children:"A friendly golden retriever who answers your questions."}),"\n",(0,a.jsx)(e.h2,{id:"bankgpt",children:"BankGPT"}),"\n",(0,a.jsx)(e.p,{children:"A customer service assistant used by a banking system."}),"\n",(0,a.jsx)(e.h2,{id:"healthgpt",children:"HealthGPT"}),"\n",(0,a.jsx)(e.p,{children:"A safety-compliant AI assistant that has strict rules against revealing sensitive internal data."})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);